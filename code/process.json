[{
  "id" : "wkkj41",
  "name" : "install_modules",
  "description" : null,
  "code" : "#!/bin/bash\n\ninstall require libaries\npip install pandas\npip install xlrd\n\n#pip install selenium\n#pip install selenium webdriver-manager\n\n\n#wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb -O ~/google-chrome-stable_current_amd64.deb\n\n#ls -lh ~/\n\n#mkdir -p ~/chrome_install\n#dpkg-deb -x ~/google-chrome-stable_current_amd64.deb ~/chrome_install\n\n#find ~/chrome_install -name \"google-chrome\"\n\n\n\n\n\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ul6ag3",
  "name" : "raw_cancer_data",
  "description" : null,
  "code" : "import os\nimport pandas as pd\nfrom download_data_utils import download_file  \n\ndef retrieve_cancer_data():\n    \"\"\"\n    Downloads the cancer incidence data and loads it into a pandas DataFrame.\n    \"\"\"\n    # Define the target directory\n    target_directory = \"/media/volume1\"\n\n    # Ensure the directory exists\n    os.makedirs(target_directory, exist_ok=True)\n\n    # Define the URL for cancer data\n    cancer_data_url = \"https://www.statecancerprofiles.cancer.gov/incidencerates/index.php?stateFIPS=00&areatype=county&cancer=047&race=00&sex=0&age=001&stage=999&type=incd&sortVariableName=rate&sortOrder=desc&output=1\"\n\n    # Define the expected filename\n    cancer_file_name = \"incd.csv\"\n    cancer_file_path = os.path.join(target_directory, cancer_file_name)\n\n    # Download the file\n    saved_file = download_file(cancer_data_url, target_directory, cancer_file_name)\n\n    if saved_file and os.path.exists(saved_file):\n        # Load the CSV file while skipping the first 8 empty rows\n        cancer_data = pd.read_csv(saved_file, skiprows=8)\n\n        # Print the first few rows\n        print(\"Cancer Data Preview:\")\n        print(cancer_data.head())  # Prints the first 5 rows\n    else:\n        print(\"Cancer data download failed. Please check the URL and permissions.\")\n\nif __name__ == \"__main__\":\n    retrieve_cancer_data()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "zmh4at",
  "name" : "cancer_data_cleaning",
  "description" : null,
  "code" : "import pandas as pd\nimport os\n\n# Define the directory where the cancer data file is stored\nTARGET_DIRECTORY = \"/media/volume1\"  \n\n# File paths\nRAW_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"incd.csv\")\nCLEANED_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"cleaned_cancer_data.csv\")\nFINAL_CLEANED_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"updated_cleaned_cancer.csv\")\n\ndef clean_cancer_data():\n    \"\"\"\n    Cleans the cancer incidence dataset by:\n    1. Removing the first 8 blank rows.\n    2. Removing unnecessary columns.\n    3. Splitting 'County' column into 'County Name' and 'State Name'.\n    4. Reordering columns for clarity.\n    5. Printing the first few rows for verification.\n    \"\"\"\n    if not os.path.exists(RAW_FILE_PATH):\n        print(f\" Error: {RAW_FILE_PATH} does not exist.\")\n        return None\n\n    try:\n        # Step 1: Read the data while skipping the first 8 rows\n        df = pd.read_csv(RAW_FILE_PATH, skiprows=8, skip_blank_lines=True)\n        print(\" Initial dataset loaded with first 8 rows removed.\")\n\n        # Step 2: Drop unnecessary columns\n        columns_to_drop = [\n            \"FIPS\", \n            \"CI*Rank([rank note])\", \n            \"Lower CI (CI*Rank)\", \n            \"Upper CI (CI*Rank)\"\n        ]\n        df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n        print(\" Unnecessary columns removed.\")\n\n        # Save intermediate cleaned dataset\n        df.to_csv(CLEANED_FILE_PATH, index=False)\n        print(f\" Cleaned dataset saved to: {CLEANED_FILE_PATH}\")\n\n        # Step 3: Further cleaning - Split 'County' into 'County Name' and 'State Name'\n        if \"County\" in df.columns:\n            df[['County Name', 'State Name']] = df['County'].str.extract(r'^(.*) County, (.*)\\(\\d+\\)$')\n            df.drop(columns=['County'], inplace=True)\n            print(\" 'County' column split into 'County Name' and 'State Name'.\")\n\n        # Step 4: Reorder columns for better readability\n        if \"State Name\" in df.columns and \"County Name\" in df.columns:\n            columns_order = [\"State Name\", \"County Name\"] + [col for col in df.columns if col not in [\"State Name\", \"County Name\"]]\n            df = df[columns_order]\n            print(\" Columns reordered.\")\n\n        # Save final cleaned dataset\n        df.to_csv(FINAL_CLEANED_FILE_PATH, index=False)\n        print(f\"\\n Final cleaned dataset saved as: {FINAL_CLEANED_FILE_PATH}\")\n\n        # Print first few rows of the cleaned dataset\n        print(\"\\n Cleaned Data Preview:\")\n        print(df.head())  # Prints the first 5 rows\n\n        return FINAL_CLEANED_FILE_PATH\n\n    except Exception as e:\n        print(f\" Error during data cleaning: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    clean_cancer_data()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ug081o",
  "name" : "radon_data_cleaning",
  "description" : null,
  "code" : "import pandas as pd\nimport os\n\n# Define the directory where the radon dataset is stored\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# Define file paths\nRAW_RADON_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"radon_zones.csv\")\nCLEANED_RADON_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"cleaned_radon_zones.csv\")\n\n# Dictionary to map state abbreviations to full names\nSTATE_ABBREVIATIONS = {\n    \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\",\n    \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DE\": \"Delaware\", \"FL\": \"Florida\", \"GA\": \"Georgia\",\n    \"HI\": \"Hawaii\", \"ID\": \"Idaho\", \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\",\n    \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\", \"MD\": \"Maryland\",\n    \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\", \"MO\": \"Missouri\",\n    \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\",\n    \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\",\n    \"OK\": \"Oklahoma\", \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\",\n    \"SD\": \"South Dakota\", \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\",\n    \"VA\": \"Virginia\", \"WA\": \"Washington\", \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\"\n}\n\ndef clean_radon_data():\n    \"\"\"\n    Cleans the Radon dataset by:\n    1. Extracting 'State' and 'County' from 'County,State' column.\n    2. Keeping only the 'State', 'County', 'Region', and 'Zone' columns.\n    3. Converting state abbreviations to full names.\n    4. Removing the 2nd and 3rd rows.\n    5. Printing the cleaned columns and first 5 rows.\n    \"\"\"\n    if not os.path.exists(RAW_RADON_FILE_PATH):\n        print(f\"Error: Radon file '{RAW_RADON_FILE_PATH}' not found.\")\n        return None\n\n    try:\n        # Load the dataset\n        df = pd.read_csv(RAW_RADON_FILE_PATH)\n\n        print(\"Initial Radon dataset loaded.\")\n\n        # Step 1: Extract 'State' and 'County' from 'County,State'\n        df[['County', 'State']] = df['County,State'].str.split(',', expand=True)\n\n        # Trim whitespace in 'State' and 'County'\n        df['State'] = df['State'].str.strip()\n        df['County'] = df['County'].str.strip()\n\n        # Step 2: Remove the 2nd and 3rd rows explicitly\n        df.drop(index=[0, 1], inplace=True)\n        df.reset_index(drop=True, inplace=True)  # Reset index after row removal\n        print(\"Removed 2nd and 3rd rows from the dataset.\")\n\n        # Step 3: Convert State abbreviations to full names\n        df['State'] = df['State'].map(STATE_ABBREVIATIONS).fillna(df['State'])\n        print(\"State abbreviations converted to full state names.\")\n\n        # Step 4: Keep only relevant columns\n        df = df[['State', 'County', 'Region', 'Zone']]\n        print(\"Kept only required columns: State, County, Region, and Zone.\")\n\n        # Save the cleaned dataset\n        df.to_csv(CLEANED_RADON_FILE_PATH, index=False)\n        print(f\"\\nCleaned Radon dataset saved as: {CLEANED_RADON_FILE_PATH}\")\n\n        # Print all columns and first 5 rows\n        print(\"\\nCleaned Data Columns:\")\n        print(df.columns.tolist())\n        print(\"\\nCleaned Data Preview:\")\n        print(df.head())\n\n        return CLEANED_RADON_FILE_PATH\n\n    except Exception as e:\n        print(f\"Error during data cleaning: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    clean_radon_data()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "f4pq53",
  "name" : "raw_radon_data",
  "description" : null,
  "code" : "import os\nimport pandas as pd\nfrom download_data_utils import download_file  \n\ndef retrieve_radon_data():\n    \"\"\"\n    Downloads the Radon Zone Data (.xls), converts it to .csv, and saves it to /media/volume1.\n    Then, prints a few rows of the converted data.\n    \"\"\"\n    # Define the target directory\n    target_directory = \"/media/volume1\"\n\n    # Ensure the directory exists\n    os.makedirs(target_directory, exist_ok=True)\n\n    # Define the URL for Radon Zone data\n    radon_data_url = \"https://www.epa.gov/system/files/documents/2024-05/radon_zones-spreadsheet.xls\"\n\n    # Define file paths\n    radon_xls_path = os.path.join(target_directory, \"radon_zones.xls\")\n    radon_csv_path = os.path.join(target_directory, \"radon_zones.csv\")\n\n    # Download the .xls file\n    saved_file = download_file(radon_data_url, target_directory, \"radon_zones.xls\")\n\n    if saved_file and os.path.exists(saved_file):\n        try:\n            # Read the .xls file into pandas\n            df = pd.read_excel(saved_file, engine=\"xlrd\")  # For .xls files\n\n            # Convert to CSV\n            df.to_csv(radon_csv_path, index=False)\n\n            print(f\"Radon Zone Data converted and saved as: {radon_csv_path}\")\n\n            # Print a few rows to verify\n            print(\"\\nRadon Zone Data Preview:\")\n            print(df.head())  # Prints the first 5 rows\n\n        except Exception as e:\n            print(f\"Error converting Radon Zone Data: {e}\")\n    else:\n        print(\"Radon Zone Data download failed. Please check the URL and permissions.\")\n\nif __name__ == \"__main__\":\n    retrieve_radon_data()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "wtszw9",
  "name" : "merged_aqi",
  "description" : null,
  "code" : "import os\nimport pandas as pd\n\n# Define the directory where the AQI files are stored\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# List of AQI CSV filenames for each year\nAQI_FILES = [\n    \"annual_aqi_by_county_2017.csv\",\n    \"annual_aqi_by_county_2018.csv\",\n    \"annual_aqi_by_county_2019.csv\",\n    \"annual_aqi_by_county_2020.csv\",\n    \"annual_aqi_by_county_2021.csv\"\n]\n\ndef get_existing_files():\n    \"\"\"\n    Checks which AQI files exist in the target directory.\n    \n    Returns:\n        existing_files (list): List of valid CSV file paths.\n        missing_files (list): List of missing CSV file paths.\n    \"\"\"\n    csv_files = [os.path.join(TARGET_DIRECTORY, file) for file in AQI_FILES]\n    \n    existing_files = [file for file in csv_files if os.path.exists(file)]\n    missing_files = [file for file in csv_files if not os.path.exists(file)]\n    \n    return existing_files, missing_files\n\ndef merge_aqi_files(existing_files):\n    \"\"\"\n    Merges the available AQI CSV files into a single dataset.\n    \n    Args:\n        existing_files (list): List of valid CSV file paths.\n\n    Returns:\n        merged_df (DataFrame): Merged Pandas DataFrame containing all AQI data.\n    \"\"\"\n    print(\"\\n Merging available AQI CSV files...\")\n\n    # Read and concatenate all existing CSV files\n    df_list = [pd.read_csv(file) for file in existing_files]\n    merged_df = pd.concat(df_list, ignore_index=True)\n\n    return merged_df\n\ndef save_merged_data(merged_df):\n    \"\"\"\n    Saves the merged AQI dataset to a CSV file.\n    \n    Args:\n        merged_df (DataFrame): Merged Pandas DataFrame.\n    \"\"\"\n    merged_file_path = os.path.join(TARGET_DIRECTORY, \"merged_annual_aqi_by_county.csv\")\n    merged_df.to_csv(merged_file_path, index=False)\n\n    print(f\"\\n Merged dataset saved as: {merged_file_path}\")\n    print(\"\\nMerged Data Preview:\")\n    print(merged_df.head())  # Print the first few rows\n\ndef main():\n    \"\"\"\n    Main function to check for available AQI files, merge them, and save the final dataset.\n    \"\"\"\n    existing_files, missing_files = get_existing_files()\n\n    if missing_files:\n        print(f\" The following files are missing and will not be included in the merge:\")\n        for missing_file in missing_files:\n            print(f\"   - {missing_file}\")\n\n    if existing_files:\n        merged_df = merge_aqi_files(existing_files)\n        save_merged_data(merged_df)\n    else:\n        print(\"\\n No valid AQI files found to merge. Please check the file locations.\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ddfkb3",
  "name" : "raw_birds_data",
  "description" : null,
  "code" : "import os\nimport pandas as pd\nfrom download_data_utils import download_file  \n\ndef retrieve_birds_data():\n    \"\"\"\n    Downloads the birds dataset and loads it into a pandas DataFrame.\n    \"\"\"\n    # Define the target directory\n    target_directory = \"/media/volume1\"\n\n    # Ensure the directory exists\n    os.makedirs(target_directory, exist_ok=True)\n\n    # Define the URL for birds data\n    birds_data_url = \"https://huggingface.co/datasets/Geoweaver/animal_data/resolve/main/hpai-wild-birds.csv?download=true\"\n\n    # Define the expected filename\n    birds_file_name = \"hpai-wild-birds.csv\"\n    birds_file_path = os.path.join(target_directory, birds_file_name)\n\n    # Download the file\n    saved_file = download_file(birds_data_url, target_directory, birds_file_name)\n\n    if saved_file and os.path.exists(saved_file):\n        # Load the CSV file\n        birds_data = pd.read_csv(saved_file)\n\n        # Print the first few rows\n        print(\"Birds Data Preview:\")\n        print(birds_data.head())  # Prints the first 5 rows\n    else:\n        print(\"Birds data download failed. Please check the URL and permissions.\")\n\nif __name__ == \"__main__\":\n    retrieve_birds_data()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "yfhedx",
  "name" : "merged_aqi_cancer",
  "description" : null,
  "code" : "import pandas as pd\nimport os\n\n# Define the directory where the cleaned datasets are stored\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# Define file paths\naqi_file_path = os.path.join(TARGET_DIRECTORY, \"merged_annual_aqi_by_county.csv\")\ncancer_file_path = os.path.join(TARGET_DIRECTORY, \"updated_cleaned_cancer.csv\")\noutput_file_path = os.path.join(TARGET_DIRECTORY, \"merged_aqi_cancer.csv\")\n\ndef merge_aqi_cancer_data():\n    \"\"\"\n    Merges the updated cleaned cancer dataset with the merged AQI dataset.\n    Expands the cancer data to match all years in the AQI dataset.\n    \"\"\"\n    if not os.path.exists(aqi_file_path):\n        print(f\" Error: AQI file '{aqi_file_path}' not found.\")\n        return None\n\n    if not os.path.exists(cancer_file_path):\n        print(f\" Error: Cancer file '{cancer_file_path}' not found.\")\n        return None\n\n    try:\n        # Load the datasets\n        aqi_df = pd.read_csv(aqi_file_path)\n        cancer_df = pd.read_csv(cancer_file_path)\n\n        print(\" Datasets loaded successfully.\")\n\n        # Standardizing column names for merging\n        cancer_df.rename(columns={'State Name': 'State', 'County Name': 'County'}, inplace=True)\n\n        # Keeping relevant columns from the cancer dataset\n        cancer_df = cancer_df[['State', 'County'] + [col for col in cancer_df.columns if col not in ['State', 'County']]]\n\n        # Extracting the unique years from the AQI dataset\n        years = aqi_df['Year'].unique()\n\n        # Expanding the cancer dataset by duplicating each row for every year in AQI dataset\n        cancer_expanded_df = cancer_df.loc[cancer_df.index.repeat(len(years))].reset_index(drop=True)\n\n        # Assigning years by repeating the entire list for the correct number of rows\n        cancer_expanded_df['Year'] = years.tolist() * (len(cancer_expanded_df) // len(years))\n\n        print(\" Cancer dataset expanded to match AQI dataset years.\")\n\n        # Merging the expanded cancer dataset with the AQI dataset\n        merged_df = pd.merge(aqi_df, cancer_expanded_df, on=['State', 'County', 'Year'], how='left')\n\n        # Save the merged dataset\n        merged_df.to_csv(output_file_path, index=False)\n\n        print(f\"\\n Merged dataset saved as: {output_file_path}\")\n        print(\"\\n Merged Data Preview:\")\n        print(merged_df.head())  # Print the first few rows\n\n        return output_file_path\n\n    except Exception as e:\n        print(f\" Error during merging: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    merge_aqi_cancer_data()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "7my0ib",
  "name" : "merged_radon_aqi_cancer",
  "description" : null,
  "code" : "import pandas as pd\nimport os\n\n# Define the directory where datasets are stored\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# Define file paths\nradon_file_path = os.path.join(TARGET_DIRECTORY, \"cleaned_radon_zones.csv\")\naqi_cancer_file_path = os.path.join(TARGET_DIRECTORY, \"merged_aqi_cancer.csv\")\noutput_file_path = os.path.join(TARGET_DIRECTORY, \"final_merged_dataset.csv\")\n\ndef merge_radon_aqi_cancer():\n    \"\"\"\n    Merges the cleaned Radon dataset with the AQI-Cancer dataset.\n    Expands the Radon data to match years in AQI-Cancer dataset.\n    Prints the total number of rows, columns, column names, and first 10 rows.\n    \"\"\"\n    if not os.path.exists(radon_file_path):\n        print(f\"Error: Radon file '{radon_file_path}' not found.\")\n        return None\n\n    if not os.path.exists(aqi_cancer_file_path):\n        print(f\"Error: AQI-Cancer file '{aqi_cancer_file_path}' not found.\")\n        return None\n\n    try:\n        # Load datasets\n        radon_df = pd.read_csv(radon_file_path)\n        aqi_cancer_df = pd.read_csv(aqi_cancer_file_path)\n\n        print(\"Datasets loaded successfully.\")\n\n        # Extract unique years from AQI-Cancer dataset\n        years = aqi_cancer_df['Year'].unique()\n\n        # Ensure Radon dataset has unique county entries\n        radon_df = radon_df.drop_duplicates(subset=['County'])\n\n        # Expand Radon data to match years in AQI-Cancer dataset\n        expanded_radon_df = radon_df.loc[radon_df.index.repeat(len(years))].reset_index(drop=True)\n        expanded_radon_df['Year'] = sorted(years.tolist() * len(radon_df))\n\n        print(\"Radon dataset expanded to match AQI-Cancer dataset years.\")\n\n        # Merge the expanded Radon dataset with the AQI-Cancer dataset on 'County' and 'Year'\n        merged_df = aqi_cancer_df.merge(expanded_radon_df, on=['County', 'Year'], how='left')\n\n        # Ensure no duplicate counties for the same year\n        merged_df = merged_df.drop_duplicates(subset=['County', 'Year'])\n\n        # Save the merged dataset\n        merged_df.to_csv(output_file_path, index=False)\n\n        # Print dataset shape\n        num_rows, num_columns = merged_df.shape\n        print(f\"\\nMerged dataset saved as: {output_file_path}\")\n        print(f\"Total Rows: {num_rows}, Total Columns: {num_columns}\")\n\n        # Print column names\n        print(\"\\nAll Columns in Merged Dataset:\")\n        print(list(merged_df.columns))\n\n        # Print first 10 rows\n        print(\"\\nFirst 10 Rows of Merged Dataset:\")\n        print(merged_df.head(10))\n\n        return output_file_path\n\n    except Exception as e:\n        print(f\"Error during merging: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    merge_radon_aqi_cancer()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "dv6ldm",
  "name" : "download_data_utils",
  "description" : null,
  "code" : "import os\nimport urllib.request\nimport urllib.parse\n\ndef download_file(url, save_location, file_name=None):\n    \"\"\"\n    Downloads a file from a given URL and saves it to the specified location.\n\n    Args:\n        url (str): The URL of the file to download.\n        save_location (str): The directory where the file should be saved.\n        file_name (str, optional): The name to save the file as. \n                                   If None, the filename is extracted from the URL.\n    \n    Returns:\n        str: Full path of the saved file if successful, None if failed.\n    \"\"\"\n    try:\n        print(\"Starting file download...\")\n\n        # Ensure the save directory exists\n        os.makedirs(save_location, exist_ok=True)\n\n        # Extract filename from URL if not provided\n        if file_name is None:\n            file_name = os.path.basename(url.split(\"?\")[0])  # Remove URL parameters\n\n        # Define the full save path\n        save_path = os.path.join(save_location, file_name)\n\n        # Open the URL and read the content\n        with urllib.request.urlopen(url) as response:\n            if response.status != 200:\n                print(f\"Failed to download file. HTTP Status: {response.status}\")\n                return None\n            file_content = response.read()\n\n        # Write the file\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n        return save_path  # Return the saved file path\n\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n        return None\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "pbr25n",
  "name" : "raw_aqi_data",
  "description" : null,
  "code" : "import os\nimport pandas as pd\nimport zipfile\nfrom download_data_utils import download_file  \n\n# Define the base directory to save files\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# Ensure the directory exists\nos.makedirs(TARGET_DIRECTORY, exist_ok=True)\n\n# Dictionary of URLs for all years\nAQI_DATA_URLS = {\n    2017: \"https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2017.zip\",\n    2018: \"https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2018.zip\",\n    2019: \"https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2019.zip\",\n    2020: \"https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2020.zip\",\n    2021: \"https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2021.zip\",\n}\n\ndef retrieve_aqi(year):\n    \"\"\"\n    Downloads the AQI ZIP file for a given year, extracts the CSV, and saves it to /media/volume1.\n    \"\"\"\n    zip_file_path = os.path.join(TARGET_DIRECTORY, f\"annual_aqi_by_county_{year}.zip\")\n    extracted_csv_path = os.path.join(TARGET_DIRECTORY, f\"annual_aqi_by_county_{year}.csv\")\n\n    # Get the download URL\n    original_url = AQI_DATA_URLS.get(year)\n    if not original_url:\n        print(f\"No URL found for year {year}. Skipping...\")\n        return\n\n    # Download the ZIP file\n    saved_file = download_file(original_url, TARGET_DIRECTORY, f\"annual_aqi_by_county_{year}.zip\")\n\n    if saved_file and os.path.exists(saved_file):\n        try:\n            # Extract the ZIP file\n            with zipfile.ZipFile(saved_file, \"r\") as zip_ref:\n                zip_ref.extractall(TARGET_DIRECTORY)\n                extracted_files = zip_ref.namelist()  # Get extracted file names\n\n            # Identify the correct CSV file\n            csv_file_name = next((f for f in extracted_files if f.endswith(\".csv\")), None)\n            if csv_file_name:\n                extracted_csv_path = os.path.join(TARGET_DIRECTORY, csv_file_name)\n\n                # Load CSV into pandas\n                df = pd.read_csv(extracted_csv_path)\n\n                # Print a few rows to verify\n                print(f\"AQI Data for {year} extracted and saved as: {extracted_csv_path}\")\n                print(\"\\nAQI Data Preview:\")\n                print(df.head())  # Prints the first 5 rows\n            else:\n                print(f\"No CSV file found in the ZIP archive for {year}.\")\n\n        except Exception as e:\n            print(f\"Error extracting or loading AQI Data for {year}: {e}\")\n    else:\n        print(f\"AQI data download failed for {year}. Please check the URL and permissions.\")\n\ndef retrieve_all_years():\n    \"\"\"\n    Downloads and processes AQI data for all specified years.\n    \"\"\"\n    for year in AQI_DATA_URLS.keys():\n        retrieve_aqi(year)\n\nif __name__ == \"__main__\":\n    retrieve_all_years()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "7eqm4n",
  "name" : "merged_data_cleaning",
  "description" : null,
  "code" : "import os\nimport pandas as pd\n\n# Define the directory where the dataset is stored\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# Define file paths\nFINAL_MERGED_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"final_merged_dataset.csv\")\nCLEANED_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"cleaned_final_merged_dataset.csv\")\n\ndef load_dataset(file_path):\n    \"\"\"\n    Loads the dataset from the given file path.\n    \n    Args:\n        file_path (str): Path to the dataset.\n\n    Returns:\n        pd.DataFrame: Loaded pandas DataFrame.\n    \"\"\"\n    if not os.path.exists(file_path):\n        print(f\"Error: File '{file_path}' not found.\")\n        return None\n\n    try:\n        df = pd.read_csv(file_path)\n        print(\"Dataset loaded successfully.\")\n        return df\n    except Exception as e:\n        print(f\"Error loading dataset: {e}\")\n        return None\n\ndef clean_dataset(df):\n    \"\"\"\n    Cleans the dataset by:\n    1. Dropping 'State_y'.\n    2. Renaming 'State_x' to 'State'.\n\n    Args:\n        df (pd.DataFrame): The dataset to clean.\n\n    Returns:\n        pd.DataFrame: Cleaned dataset.\n    \"\"\"\n    try:\n        # Drop 'State_y' if it exists\n        if \"State_y\" in df.columns:\n            df.drop(columns=[\"State_y\"], inplace=True)\n\n        # Rename 'State_x' to 'State' if it exists\n        if \"State_x\" in df.columns:\n            df.rename(columns={\"State_x\": \"State\"}, inplace=True)\n\n        print(\"Dataset cleaned successfully.\")\n        return df\n    except Exception as e:\n        print(f\"Error during cleaning: {e}\")\n        return None\n\ndef save_cleaned_dataset(df, file_path):\n    \"\"\"\n    Saves the cleaned dataset to the specified file path.\n\n    Args:\n        df (pd.DataFrame): The cleaned dataset.\n        file_path (str): Path to save the cleaned dataset.\n    \"\"\"\n    try:\n        df.to_csv(file_path, index=False)\n        print(f\"Cleaned dataset saved successfully as '{file_path}'.\")\n    except Exception as e:\n        print(f\"Error saving dataset: {e}\")\n\ndef print_dataset_info(df):\n    \"\"\"\n    Prints the dataset's column names and first 10 rows.\n\n    Args:\n        df (pd.DataFrame): The dataset to print.\n    \"\"\"\n    print(\"\\nAll Columns in Cleaned Dataset:\")\n    print(list(df.columns))\n\n    print(\"\\nFirst 10 Rows of Cleaned Dataset:\")\n    print(df.head(10))\n\ndef main():\n    \"\"\"\n    Main function to load, clean, and save the dataset.\n    \"\"\"\n    df = load_dataset(FINAL_MERGED_FILE_PATH)\n    if df is not None:\n        cleaned_df = clean_dataset(df)\n        if cleaned_df is not None:\n            save_cleaned_dataset(cleaned_df, CLEANED_FILE_PATH)\n            print_dataset_info(cleaned_df)  # Print columns and first 10 rows\n\nif __name__ == \"__main__\":\n    main()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "kth1gk",
  "name" : "birds_cleaned_data",
  "description" : null,
  "code" : "import os\nimport pandas as pd\n\n# Define the directory where the dataset is stored\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# Define file paths\nBIRDS_DATA_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"hpai-wild-birds.csv\")\nCLEANED_BIRDS_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"hpai-wild-birds-2022.csv\")\n\ndef load_dataset(file_path):\n    \"\"\"\n    Loads the dataset from the given file path.\n\n    Args:\n        file_path (str): Path to the dataset.\n\n    Returns:\n        pd.DataFrame: Loaded pandas DataFrame.\n    \"\"\"\n    if not os.path.exists(file_path):\n        print(f\"Error: File '{file_path}' not found.\")\n        return None\n\n    try:\n        df = pd.read_csv(file_path)\n        print(\"Dataset loaded successfully.\")\n        return df\n    except Exception as e:\n        print(f\"Error loading dataset: {e}\")\n        return None\n\ndef filter_by_year(df, year=2022):\n    \"\"\"\n    Filters the dataset to keep only rows where 'Date Detected' is in the specified year.\n\n    Args:\n        df (pd.DataFrame): The dataset to filter.\n        year (int): The year to keep.\n\n    Returns:\n        pd.DataFrame: Filtered dataset.\n    \"\"\"\n    try:\n        # Ensure 'Date Detected' column exists\n        if \"Date Detected\" not in df.columns:\n            print(\"Error: 'Date Detected' column not found in dataset.\")\n            return None\n\n        # Convert 'Date Detected' column to datetime format\n        df[\"Date Detected\"] = pd.to_datetime(df[\"Date Detected\"], errors=\"coerce\")\n\n        # Filter only rows where the year is 2022\n        df_filtered = df[df[\"Date Detected\"].dt.year == year]\n\n        print(f\"Dataset filtered to keep only rows from {year}.\")\n        return df_filtered\n    except Exception as e:\n        print(f\"Error during filtering: {e}\")\n        return None\n\ndef save_filtered_dataset(df, file_path):\n    \"\"\"\n    Saves the filtered dataset to the specified file path.\n\n    Args:\n        df (pd.DataFrame): The filtered dataset.\n        file_path (str): Path to save the dataset.\n    \"\"\"\n    try:\n        df.to_csv(file_path, index=False)\n        print(f\"Filtered dataset saved successfully as '{file_path}'.\")\n    except Exception as e:\n        print(f\"Error saving dataset: {e}\")\n\ndef print_dataset_info(df):\n    \"\"\"\n    Prints the dataset's column names, first 10 rows, and total number of rows and columns.\n\n    Args:\n        df (pd.DataFrame): The dataset to print.\n    \"\"\"\n    num_rows, num_columns = df.shape\n    print(f\"\\nTotal Rows: {num_rows}, Total Columns: {num_columns}\")\n\n    print(\"\\nAll Columns in Filtered Dataset:\")\n    print(list(df.columns))\n\n    print(\"\\nFirst 10 Rows of Filtered Dataset:\")\n    print(df.head(10))\n\ndef main():\n    \"\"\"\n    Main function to load, filter, save, and print dataset information.\n    \"\"\"\n    df = load_dataset(BIRDS_DATA_FILE_PATH)\n    if df is not None:\n        filtered_df = filter_by_year(df, 2022)\n        if filtered_df is not None:\n            save_filtered_dataset(filtered_df, CLEANED_BIRDS_FILE_PATH)\n            print_dataset_info(filtered_df)  # Print columns, first 10 rows, and dataset size\n\nif __name__ == \"__main__\":\n    main()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "4h36mr",
  "name" : "final_merged_data",
  "description" : null,
  "code" : "import os\nimport pandas as pd\n\n# Define the directory where datasets are stored\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# Define file paths\nCLEANED_FINAL_DATA_PATH = os.path.join(TARGET_DIRECTORY, \"cleaned_final_merged_dataset.csv\")\nBIRDS_DATA_PATH = os.path.join(TARGET_DIRECTORY, \"hpai-wild-birds-2022.csv\")\nOUTPUT_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"final_merged_with_birds.csv\")\n\ndef load_dataset(file_path):\n    \"\"\"\n    Loads the dataset from the given file path.\n\n    Args:\n        file_path (str): Path to the dataset.\n\n    Returns:\n        pd.DataFrame: Loaded pandas DataFrame.\n    \"\"\"\n    if not os.path.exists(file_path):\n        print(f\"Error: File '{file_path}' not found.\")\n        return None\n\n    try:\n        df = pd.read_csv(file_path)\n        print(f\"Dataset loaded successfully from {file_path}\")\n        return df\n    except Exception as e:\n        print(f\"Error loading dataset: {e}\")\n        return None\n\ndef merge_datasets(aqi_cancer_df, birds_df):\n    \"\"\"\n    Merges the AQI-Cancer dataset with the Birds dataset based on 'County'.\n    Retains the 'State' column from AQI-Cancer data.\n    Removes duplicate 'County' and 'State' columns from the Birds dataset.\n\n    Args:\n        aqi_cancer_df (pd.DataFrame): The cleaned AQI-Cancer dataset.\n        birds_df (pd.DataFrame): The birds dataset.\n\n    Returns:\n        pd.DataFrame: Merged dataset.\n    \"\"\"\n    try:\n        # Keep a copy of the State column from AQI-Cancer dataset\n        if \"State\" in aqi_cancer_df.columns:\n            state_map = aqi_cancer_df.set_index(\"County\")[\"State\"].to_dict()\n        \n        # Merge datasets on 'County', keeping all columns (fill missing values with NaN)\n        merged_df = aqi_cancer_df.merge(birds_df, on=\"County\", how=\"left\")\n\n        # Restore the 'State' column from AQI-Cancer dataset\n        if \"State\" in aqi_cancer_df.columns:\n            merged_df[\"State\"] = merged_df[\"County\"].map(state_map)\n\n        # Drop the duplicate 'County' and 'State' columns from the Birds dataset\n        columns_to_drop = [col for col in [\"State_x\", \"State_y\"] if col in merged_df.columns]\n        merged_df.drop(columns=columns_to_drop, errors=\"ignore\", inplace=True)\n\n        print(\"Datasets merged successfully.\")\n        return merged_df\n    except Exception as e:\n        print(f\"Error during merging: {e}\")\n        return None\n\ndef reorder_columns(df):\n    \"\"\"\n    Moves the 'State' column to the first position.\n\n    Args:\n        df (pd.DataFrame): The dataset to reorder.\n\n    Returns:\n        pd.DataFrame: Dataset with 'State' as the first column.\n    \"\"\"\n    if \"State\" in df.columns:\n        column_order = [\"State\"] + [col for col in df.columns if col != \"State\"]\n        df = df[column_order]\n        print(\"Moved 'State' column to the first position.\")\n    else:\n        print(\"Warning: 'State' column not found in dataset.\")\n    \n    return df\n\ndef save_dataset(df, file_path):\n    \"\"\"\n    Saves the merged dataset to the specified file path.\n\n    Args:\n        df (pd.DataFrame): The merged dataset.\n        file_path (str): Path to save the dataset.\n    \"\"\"\n    try:\n        df.to_csv(file_path, index=False)\n        print(f\"Merged dataset saved successfully as '{file_path}'.\")\n    except Exception as e:\n        print(f\"Error saving dataset: {e}\")\n\ndef print_dataset_info(df):\n    \"\"\"\n    Prints the dataset's column names, first 10 rows, and total number of rows and columns.\n\n    Args:\n        df (pd.DataFrame): The dataset to print.\n    \"\"\"\n    num_rows, num_columns = df.shape\n    print(f\"\\nTotal Rows: {num_rows}, Total Columns: {num_columns}\")\n\n    print(\"\\nAll Columns in Merged Dataset:\")\n    print(list(df.columns))\n\n    print(\"\\nFirst 10 Rows of Merged Dataset:\")\n    print(df.head(10))\n\ndef main():\n    \"\"\"\n    Main function to load, merge, reorder, save, and print dataset information.\n    \"\"\"\n    aqi_cancer_df = load_dataset(CLEANED_FINAL_DATA_PATH)\n    birds_df = load_dataset(BIRDS_DATA_PATH)\n\n    if aqi_cancer_df is not None and birds_df is not None:\n        merged_df = merge_datasets(aqi_cancer_df, birds_df)\n        if merged_df is not None:\n            merged_df = reorder_columns(merged_df)  # Move 'State' column to first position\n            save_dataset(merged_df, OUTPUT_FILE_PATH)\n            print_dataset_info(merged_df)  # Print dataset info after merging\n\nif __name__ == \"__main__\":\n    main()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
}]