[{
  "history_id" : "s6s4489ygpf",
  "history_input" : "#!/bin/bash\n\ninstall require libaries\npip install pandas\npip install xlrd\n\n#pip install selenium\n#pip install selenium webdriver-manager\n\n\n#wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb -O ~/google-chrome-stable_current_amd64.deb\n\n#ls -lh ~/\n\n#mkdir -p ~/chrome_install\n#dpkg-deb -x ~/google-chrome-stable_current_amd64.deb ~/chrome_install\n\n#find ~/chrome_install -name \"google-chrome\"\n\n\n\n\n\n\n",
  "history_output" : "install: cannot stat 'require': No such file or directory\nRequirement already satisfied: pandas in /home/geo2021/anaconda3/lib/python3.12/site-packages (2.2.3)\nRequirement already satisfied: numpy>=1.26.0 in /home/geo2021/anaconda3/lib/python3.12/site-packages (from pandas) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /home/geo2021/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /home/geo2021/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /home/geo2021/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\nRequirement already satisfied: six>=1.5 in /home/geo2021/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\nRequirement already satisfied: xlrd in /home/geo2021/anaconda3/lib/python3.12/site-packages (2.0.1)\n",
  "history_begin_time" : 1741882802175,
  "history_end_time" : 1741882814947,
  "history_notes" : null,
  "history_process" : "wkkj41",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "bjxnvo64jui",
  "history_input" : "import os\nimport pandas as pd\nfrom download_data_utils import download_file  \n\ndef retrieve_cancer_data():\n    \"\"\"\n    Downloads the cancer incidence data and loads it into a pandas DataFrame.\n    \"\"\"\n    # Define the target directory\n    target_directory = \"/media/volume1\"\n\n    # Ensure the directory exists\n    os.makedirs(target_directory, exist_ok=True)\n\n    # Define the URL for cancer data\n    cancer_data_url = \"https://www.statecancerprofiles.cancer.gov/incidencerates/index.php?stateFIPS=00&areatype=county&cancer=047&race=00&sex=0&age=001&stage=999&type=incd&sortVariableName=rate&sortOrder=desc&output=1\"\n\n    # Define the expected filename\n    cancer_file_name = \"incd.csv\"\n    cancer_file_path = os.path.join(target_directory, cancer_file_name)\n\n    # Download the file\n    saved_file = download_file(cancer_data_url, target_directory, cancer_file_name)\n\n    if saved_file and os.path.exists(saved_file):\n        # Load the CSV file while skipping the first 8 empty rows\n        cancer_data = pd.read_csv(saved_file, skiprows=8)\n\n        # Print the first few rows\n        print(\"Cancer Data Preview:\")\n        print(cancer_data.head())  # Prints the first 5 rows\n    else:\n        print(\"Cancer data download failed. Please check the URL and permissions.\")\n\nif __name__ == \"__main__\":\n    retrieve_cancer_data()\n",
  "history_output" : "Starting file download...\nFile downloaded successfully and saved as: /media/volume1/incd.csv\nCancer Data Preview:\n                          County  ...  Upper 95% Confidence Interval.1\n0              US (SEER+NPCR)(1)  ...                             -2.9\n1       Union County, Florida(6)  ...                             -0.7\n2     Owsley County, Kentucky(7)  ...                              4.3\n3  Alexander County, Illinois(7)  ...                              2.1\n4    Carroll County, Kentucky(7)  ...                              2.6\n[5 rows x 14 columns]\n",
  "history_begin_time" : 1741882815530,
  "history_end_time" : 1741882816645,
  "history_notes" : null,
  "history_process" : "ul6ag3",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ztvshk0qsia",
  "history_input" : "import pandas as pd\nimport os\n\n# Define the directory where the cancer data file is stored\nTARGET_DIRECTORY = \"/media/volume1\"  \n\n# File paths\nRAW_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"incd.csv\")\nCLEANED_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"cleaned_cancer_data.csv\")\nFINAL_CLEANED_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"updated_cleaned_cancer.csv\")\n\ndef clean_cancer_data():\n    \"\"\"\n    Cleans the cancer incidence dataset by:\n    1. Removing the first 8 blank rows.\n    2. Removing unnecessary columns.\n    3. Splitting 'County' column into 'County Name' and 'State Name'.\n    4. Reordering columns for clarity.\n    5. Printing the first few rows for verification.\n    \"\"\"\n    if not os.path.exists(RAW_FILE_PATH):\n        print(f\" Error: {RAW_FILE_PATH} does not exist.\")\n        return None\n\n    try:\n        # Step 1: Read the data while skipping the first 8 rows\n        df = pd.read_csv(RAW_FILE_PATH, skiprows=8, skip_blank_lines=True)\n        print(\" Initial dataset loaded with first 8 rows removed.\")\n\n        # Step 2: Drop unnecessary columns\n        columns_to_drop = [\n            \"FIPS\", \n            \"CI*Rank([rank note])\", \n            \"Lower CI (CI*Rank)\", \n            \"Upper CI (CI*Rank)\"\n        ]\n        df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n        print(\" Unnecessary columns removed.\")\n\n        # Save intermediate cleaned dataset\n        df.to_csv(CLEANED_FILE_PATH, index=False)\n        print(f\" Cleaned dataset saved to: {CLEANED_FILE_PATH}\")\n\n        # Step 3: Further cleaning - Split 'County' into 'County Name' and 'State Name'\n        if \"County\" in df.columns:\n            df[['County Name', 'State Name']] = df['County'].str.extract(r'^(.*) County, (.*)\\(\\d+\\)$')\n            df.drop(columns=['County'], inplace=True)\n            print(\" 'County' column split into 'County Name' and 'State Name'.\")\n\n        # Step 4: Reorder columns for better readability\n        if \"State Name\" in df.columns and \"County Name\" in df.columns:\n            columns_order = [\"State Name\", \"County Name\"] + [col for col in df.columns if col not in [\"State Name\", \"County Name\"]]\n            df = df[columns_order]\n            print(\" Columns reordered.\")\n\n        # Save final cleaned dataset\n        df.to_csv(FINAL_CLEANED_FILE_PATH, index=False)\n        print(f\"\\n Final cleaned dataset saved as: {FINAL_CLEANED_FILE_PATH}\")\n\n        # Print first few rows of the cleaned dataset\n        print(\"\\n Cleaned Data Preview:\")\n        print(df.head())  # Prints the first 5 rows\n\n        return FINAL_CLEANED_FILE_PATH\n\n    except Exception as e:\n        print(f\" Error during data cleaning: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    clean_cancer_data()\n",
  "history_output" : " Initial dataset loaded with first 8 rows removed.\n Unnecessary columns removed.\n Cleaned dataset saved to: /media/volume1/cleaned_cancer_data.csv\n 'County' column split into 'County Name' and 'State Name'.\n Columns reordered.\n Final cleaned dataset saved as: /media/volume1/updated_cleaned_cancer.csv\n Cleaned Data Preview:\n  State Name  ... Upper 95% Confidence Interval.1\n0        NaN  ...                            -2.9\n1    Florida  ...                            -0.7\n2   Kentucky  ...                             4.3\n3   Illinois  ...                             2.1\n4   Kentucky  ...                             2.6\n[5 rows x 11 columns]\n",
  "history_begin_time" : 1741882817449,
  "history_end_time" : 1741882818061,
  "history_notes" : null,
  "history_process" : "zmh4at",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "l1lr9v6pmbr",
  "history_input" : "import pandas as pd\nimport os\n\n# Define the directory where the radon dataset is stored\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# Define file paths\nRAW_RADON_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"radon_zones.csv\")\nCLEANED_RADON_FILE_PATH = os.path.join(TARGET_DIRECTORY, \"cleaned_radon_zones.csv\")\n\n# Dictionary to map state abbreviations to full names\nSTATE_ABBREVIATIONS = {\n    \"AL\": \"Alabama\", \"AK\": \"Alaska\", \"AZ\": \"Arizona\", \"AR\": \"Arkansas\", \"CA\": \"California\",\n    \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"DE\": \"Delaware\", \"FL\": \"Florida\", \"GA\": \"Georgia\",\n    \"HI\": \"Hawaii\", \"ID\": \"Idaho\", \"IL\": \"Illinois\", \"IN\": \"Indiana\", \"IA\": \"Iowa\",\n    \"KS\": \"Kansas\", \"KY\": \"Kentucky\", \"LA\": \"Louisiana\", \"ME\": \"Maine\", \"MD\": \"Maryland\",\n    \"MA\": \"Massachusetts\", \"MI\": \"Michigan\", \"MN\": \"Minnesota\", \"MS\": \"Mississippi\", \"MO\": \"Missouri\",\n    \"MT\": \"Montana\", \"NE\": \"Nebraska\", \"NV\": \"Nevada\", \"NH\": \"New Hampshire\", \"NJ\": \"New Jersey\",\n    \"NM\": \"New Mexico\", \"NY\": \"New York\", \"NC\": \"North Carolina\", \"ND\": \"North Dakota\", \"OH\": \"Ohio\",\n    \"OK\": \"Oklahoma\", \"OR\": \"Oregon\", \"PA\": \"Pennsylvania\", \"RI\": \"Rhode Island\", \"SC\": \"South Carolina\",\n    \"SD\": \"South Dakota\", \"TN\": \"Tennessee\", \"TX\": \"Texas\", \"UT\": \"Utah\", \"VT\": \"Vermont\",\n    \"VA\": \"Virginia\", \"WA\": \"Washington\", \"WV\": \"West Virginia\", \"WI\": \"Wisconsin\", \"WY\": \"Wyoming\"\n}\n\ndef clean_radon_data():\n    \"\"\"\n    Cleans the Radon dataset by:\n    1. Extracting 'State' and 'County' from 'County,State' column.\n    2. Keeping only the 'State', 'County', 'Region', and 'Zone' columns.\n    3. Converting state abbreviations to full names.\n    4. Removing the 2nd and 3rd rows.\n    5. Printing the cleaned columns and first 5 rows.\n    \"\"\"\n    if not os.path.exists(RAW_RADON_FILE_PATH):\n        print(f\"Error: Radon file '{RAW_RADON_FILE_PATH}' not found.\")\n        return None\n\n    try:\n        # Load the dataset\n        df = pd.read_csv(RAW_RADON_FILE_PATH)\n\n        print(\"Initial Radon dataset loaded.\")\n\n        # Step 1: Extract 'State' and 'County' from 'County,State'\n        df[['County', 'State']] = df['County,State'].str.split(',', expand=True)\n\n        # Trim whitespace in 'State' and 'County'\n        df['State'] = df['State'].str.strip()\n        df['County'] = df['County'].str.strip()\n\n        # Step 2: Remove the 2nd and 3rd rows explicitly\n        df.drop(index=[0, 1], inplace=True)\n        df.reset_index(drop=True, inplace=True)  # Reset index after row removal\n        print(\"Removed 2nd and 3rd rows from the dataset.\")\n\n        # Step 3: Convert State abbreviations to full names\n        df['State'] = df['State'].map(STATE_ABBREVIATIONS).fillna(df['State'])\n        print(\"State abbreviations converted to full state names.\")\n\n        # Step 4: Keep only relevant columns\n        df = df[['State', 'County', 'Region', 'Zone']]\n        print(\"Kept only required columns: State, County, Region, and Zone.\")\n\n        # Save the cleaned dataset\n        df.to_csv(CLEANED_RADON_FILE_PATH, index=False)\n        print(f\"\\nCleaned Radon dataset saved as: {CLEANED_RADON_FILE_PATH}\")\n\n        # Print all columns and first 5 rows\n        print(\"\\nCleaned Data Columns:\")\n        print(df.columns.tolist())\n        print(\"\\nCleaned Data Preview:\")\n        print(df.head())\n\n        return CLEANED_RADON_FILE_PATH\n\n    except Exception as e:\n        print(f\"Error during data cleaning: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    clean_radon_data()\n",
  "history_output" : "Initial Radon dataset loaded.\nRemoved 2nd and 3rd rows from the dataset.\nState abbreviations converted to full state names.\nKept only required columns: State, County, Region, and Zone.\nCleaned Radon dataset saved as: /media/volume1/cleaned_radon_zones.csv\nCleaned Data Columns:\n['State', 'County', 'Region', 'Zone']\nCleaned Data Preview:\n     State   County Region Zone\n0  Alabama  Autauga      4    2\n1  Alabama  Baldwin      4    3\n2  Alabama  Barbour      4    2\n3  Alabama     Bibb      4    2\n4  Alabama   Blount      4    2\n",
  "history_begin_time" : 1741882818147,
  "history_end_time" : 1741882818778,
  "history_notes" : null,
  "history_process" : "ug081o",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "1zs6p7lkc56",
  "history_input" : "import os\nimport pandas as pd\nfrom download_data_utils import download_file  \n\ndef retrieve_radon_data():\n    \"\"\"\n    Downloads the Radon Zone Data (.xls), converts it to .csv, and saves it to /media/volume1.\n    Then, prints a few rows of the converted data.\n    \"\"\"\n    # Define the target directory\n    target_directory = \"/media/volume1\"\n\n    # Ensure the directory exists\n    os.makedirs(target_directory, exist_ok=True)\n\n    # Define the URL for Radon Zone data\n    radon_data_url = \"https://www.epa.gov/system/files/documents/2024-05/radon_zones-spreadsheet.xls\"\n\n    # Define file paths\n    radon_xls_path = os.path.join(target_directory, \"radon_zones.xls\")\n    radon_csv_path = os.path.join(target_directory, \"radon_zones.csv\")\n\n    # Download the .xls file\n    saved_file = download_file(radon_data_url, target_directory, \"radon_zones.xls\")\n\n    if saved_file and os.path.exists(saved_file):\n        try:\n            # Read the .xls file into pandas\n            df = pd.read_excel(saved_file, engine=\"xlrd\")  # For .xls files\n\n            # Convert to CSV\n            df.to_csv(radon_csv_path, index=False)\n\n            print(f\"Radon Zone Data converted and saved as: {radon_csv_path}\")\n\n            # Print a few rows to verify\n            print(\"\\nRadon Zone Data Preview:\")\n            print(df.head())  # Prints the first 5 rows\n\n        except Exception as e:\n            print(f\"Error converting Radon Zone Data: {e}\")\n    else:\n        print(\"Radon Zone Data download failed. Please check the URL and permissions.\")\n\nif __name__ == \"__main__\":\n    retrieve_radon_data()\n",
  "history_output" : "Starting file download...\nFile downloaded successfully and saved as: /media/volume1/radon_zones.xls\nRadon Zone Data converted and saved as: /media/volume1/radon_zones.csv\nRadon Zone Data Preview:\n    County,State     COUNTY LABEL    STATE  ... Zone Unnamed: 5  EPA 402/A-16/001\n0  UNITED STATES          no data  no data  ...    .        NaN               NaN\n1        ALABAMA          no data  no data  ...    .        NaN               NaN\n2    Autauga, AL  .Autauga County  Alabama  ...    2        NaN               NaN\n3    Baldwin, AL  .Baldwin County  Alabama  ...    3        NaN               NaN\n4    Barbour, AL  .Barbour County  Alabama  ...    2        NaN               NaN\n[5 rows x 7 columns]\n",
  "history_begin_time" : 1741882815530,
  "history_end_time" : 1741882816941,
  "history_notes" : null,
  "history_process" : "f4pq53",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "0nsrqe8lkef",
  "history_input" : "import os\nimport pandas as pd\n\n# Define the directory where the AQI files are stored\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# List of AQI CSV filenames for each year\nAQI_FILES = [\n    \"annual_aqi_by_county_2017.csv\",\n    \"annual_aqi_by_county_2018.csv\",\n    \"annual_aqi_by_county_2019.csv\",\n    \"annual_aqi_by_county_2020.csv\",\n    \"annual_aqi_by_county_2021.csv\"\n]\n\ndef get_existing_files():\n    \"\"\"\n    Checks which AQI files exist in the target directory.\n    \n    Returns:\n        existing_files (list): List of valid CSV file paths.\n        missing_files (list): List of missing CSV file paths.\n    \"\"\"\n    csv_files = [os.path.join(TARGET_DIRECTORY, file) for file in AQI_FILES]\n    \n    existing_files = [file for file in csv_files if os.path.exists(file)]\n    missing_files = [file for file in csv_files if not os.path.exists(file)]\n    \n    return existing_files, missing_files\n\ndef merge_aqi_files(existing_files):\n    \"\"\"\n    Merges the available AQI CSV files into a single dataset.\n    \n    Args:\n        existing_files (list): List of valid CSV file paths.\n\n    Returns:\n        merged_df (DataFrame): Merged Pandas DataFrame containing all AQI data.\n    \"\"\"\n    print(\"\\n Merging available AQI CSV files...\")\n\n    # Read and concatenate all existing CSV files\n    df_list = [pd.read_csv(file) for file in existing_files]\n    merged_df = pd.concat(df_list, ignore_index=True)\n\n    return merged_df\n\ndef save_merged_data(merged_df):\n    \"\"\"\n    Saves the merged AQI dataset to a CSV file.\n    \n    Args:\n        merged_df (DataFrame): Merged Pandas DataFrame.\n    \"\"\"\n    merged_file_path = os.path.join(TARGET_DIRECTORY, \"merged_annual_aqi_by_county.csv\")\n    merged_df.to_csv(merged_file_path, index=False)\n\n    print(f\"\\n Merged dataset saved as: {merged_file_path}\")\n    print(\"\\nMerged Data Preview:\")\n    print(merged_df.head())  # Print the first few rows\n\ndef main():\n    \"\"\"\n    Main function to check for available AQI files, merge them, and save the final dataset.\n    \"\"\"\n    existing_files, missing_files = get_existing_files()\n\n    if missing_files:\n        print(f\" The following files are missing and will not be included in the merge:\")\n        for missing_file in missing_files:\n            print(f\"   - {missing_file}\")\n\n    if existing_files:\n        merged_df = merge_aqi_files(existing_files)\n        save_merged_data(merged_df)\n    else:\n        print(\"\\n No valid AQI files found to merge. Please check the file locations.\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "history_output" : " Merging available AQI CSV files...\n Merged dataset saved as: /media/volume1/merged_annual_aqi_by_county.csv\nMerged Data Preview:\n     State   County  Year  ...  Days Ozone  Days PM2.5  Days PM10\n0  Alabama  Baldwin  2017  ...         188          82          0\n1  Alabama     Clay  2017  ...           0         118          0\n2  Alabama  Colbert  2017  ...         202          81          0\n3  Alabama   DeKalb  2017  ...         290          69          0\n4  Alabama   Elmore  2017  ...         226           0          0\n[5 rows x 18 columns]\n",
  "history_begin_time" : 1741882821281,
  "history_end_time" : 1741882822001,
  "history_notes" : null,
  "history_process" : "wtszw9",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "edikdxjy9kj",
  "history_input" : "import pandas as pd\nimport os\nfrom download_data_utils. import download_file\n\ndef retrieve_hpai():\n    # Define the path to the Downloads folder\n    downloads_path = os.path.expanduser(\"~/Downloads\")\n\n    original_url = \"https://xxx\"\n\n    # Define the full path to the file\n    hpai_file = os.path.join(downloads_path, \"HPAI Detections in Wild Birds.csv\")\n\n    # automatically download it to the file\n    download_file(original_url, hpai_file)\n\n    # Load the CSV file\n    hpai_data = pd.read_csv(hpai_file)\n\n    # Print the first few rows\n    print(hpai_data.head())  # Prints the first 5 rows\n\nif __name__ == \"__main__\":\n    retrieve_hpai()\n",
  "history_output" : "  File \"/media/volume1/gw-workspace/edikdxjy9kj/raw_birds_data.py\", line 3\n    from download_data_utils. import download_file\n                              ^^^^^^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1741882819444,
  "history_end_time" : 1741882819509,
  "history_notes" : null,
  "history_process" : "ddfkb3",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ylsb4gwmt7a",
  "history_input" : "import pandas as pd\nimport os\n\n# Define the directory where the cleaned datasets are stored\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# Define file paths\naqi_file_path = os.path.join(TARGET_DIRECTORY, \"merged_annual_aqi_by_county.csv\")\ncancer_file_path = os.path.join(TARGET_DIRECTORY, \"updated_cleaned_cancer.csv\")\noutput_file_path = os.path.join(TARGET_DIRECTORY, \"merged_aqi_cancer.csv\")\n\ndef merge_aqi_cancer_data():\n    \"\"\"\n    Merges the updated cleaned cancer dataset with the merged AQI dataset.\n    Expands the cancer data to match all years in the AQI dataset.\n    \"\"\"\n    if not os.path.exists(aqi_file_path):\n        print(f\" Error: AQI file '{aqi_file_path}' not found.\")\n        return None\n\n    if not os.path.exists(cancer_file_path):\n        print(f\" Error: Cancer file '{cancer_file_path}' not found.\")\n        return None\n\n    try:\n        # Load the datasets\n        aqi_df = pd.read_csv(aqi_file_path)\n        cancer_df = pd.read_csv(cancer_file_path)\n\n        print(\" Datasets loaded successfully.\")\n\n        # Standardizing column names for merging\n        cancer_df.rename(columns={'State Name': 'State', 'County Name': 'County'}, inplace=True)\n\n        # Keeping relevant columns from the cancer dataset\n        cancer_df = cancer_df[['State', 'County'] + [col for col in cancer_df.columns if col not in ['State', 'County']]]\n\n        # Extracting the unique years from the AQI dataset\n        years = aqi_df['Year'].unique()\n\n        # Expanding the cancer dataset by duplicating each row for every year in AQI dataset\n        cancer_expanded_df = cancer_df.loc[cancer_df.index.repeat(len(years))].reset_index(drop=True)\n\n        # Assigning years by repeating the entire list for the correct number of rows\n        cancer_expanded_df['Year'] = years.tolist() * (len(cancer_expanded_df) // len(years))\n\n        print(\" Cancer dataset expanded to match AQI dataset years.\")\n\n        # Merging the expanded cancer dataset with the AQI dataset\n        merged_df = pd.merge(aqi_df, cancer_expanded_df, on=['State', 'County', 'Year'], how='left')\n\n        # Save the merged dataset\n        merged_df.to_csv(output_file_path, index=False)\n\n        print(f\"\\n Merged dataset saved as: {output_file_path}\")\n        print(\"\\n Merged Data Preview:\")\n        print(merged_df.head())  # Print the first few rows\n\n        return output_file_path\n\n    except Exception as e:\n        print(f\" Error during merging: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    merge_aqi_cancer_data()\n",
  "history_output" : " Datasets loaded successfully.\n Cancer dataset expanded to match AQI dataset years.\n Merged dataset saved as: /media/volume1/merged_aqi_cancer.csv\n Merged Data Preview:\n     State  ... Upper 95% Confidence Interval.1\n0  Alabama  ...                            -1.4\n1  Alabama  ...                             1.1\n2  Alabama  ...                             1.3\n3  Alabama  ...                            -0.5\n4  Alabama  ...                            -0.8\n[5 rows x 27 columns]\n",
  "history_begin_time" : 1741882823284,
  "history_end_time" : 1741882823946,
  "history_notes" : null,
  "history_process" : "yfhedx",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ypr3ng12f3d",
  "history_input" : "import pandas as pd\nimport os\n\n# Define the directory where datasets are stored\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# Define file paths\nradon_file_path = os.path.join(TARGET_DIRECTORY, \"cleaned_radon_zones.csv\")\naqi_cancer_file_path = os.path.join(TARGET_DIRECTORY, \"merged_aqi_cancer.csv\")\noutput_file_path = os.path.join(TARGET_DIRECTORY, \"final_merged_dataset.csv\")\n\ndef merge_radon_aqi_cancer():\n    \"\"\"\n    Merges the cleaned Radon dataset with the AQI-Cancer dataset.\n    Expands the Radon data to match years in AQI-Cancer dataset.\n    Prints the total number of rows and columns.\n    \"\"\"\n    if not os.path.exists(radon_file_path):\n        print(f\"Error: Radon file '{radon_file_path}' not found.\")\n        return None\n\n    if not os.path.exists(aqi_cancer_file_path):\n        print(f\"Error: AQI-Cancer file '{aqi_cancer_file_path}' not found.\")\n        return None\n\n    try:\n        # Load datasets\n        radon_df = pd.read_csv(radon_file_path)\n        aqi_cancer_df = pd.read_csv(aqi_cancer_file_path)\n\n        print(\"Datasets loaded successfully.\")\n\n        # Extract unique years from AQI-Cancer dataset\n        years = aqi_cancer_df['Year'].unique()\n\n        # Ensure Radon dataset has unique county entries\n        radon_df = radon_df.drop_duplicates(subset=['County'])\n\n        # Expand Radon data to match years in AQI-Cancer dataset\n        expanded_radon_df = radon_df.loc[radon_df.index.repeat(len(years))].reset_index(drop=True)\n        expanded_radon_df['Year'] = sorted(years.tolist() * len(radon_df))\n\n        print(\"Radon dataset expanded to match AQI-Cancer dataset years.\")\n\n        # Merge the expanded Radon dataset with the AQI-Cancer dataset on 'County' and 'Year'\n        merged_df = aqi_cancer_df.merge(expanded_radon_df, on=['County', 'Year'], how='left')\n\n        # Ensure no duplicate counties for the same year\n        merged_df = merged_df.drop_duplicates(subset=['County', 'Year'])\n\n        # Save the merged dataset\n        merged_df.to_csv(output_file_path, index=False)\n\n        # Print dataset shape\n        num_rows, num_columns = merged_df.shape\n        print(f\"\\nMerged dataset saved as: {output_file_path}\")\n        print(f\"Total Rows: {num_rows}, Total Columns: {num_columns}\")\n\n        print(\"\\nMerged Data Preview:\")\n        print(merged_df.head())\n\n        return output_file_path\n\n    except Exception as e:\n        print(f\"Error during merging: {e}\")\n        return None\n\nif __name__ == \"__main__\":\n    merge_radon_aqi_cancer()\n",
  "history_output" : "Datasets loaded successfully.\nRadon dataset expanded to match AQI-Cancer dataset years.\nMerged dataset saved as: /media/volume1/final_merged_dataset.csv\nTotal Rows: 3941, Total Columns: 30\nMerged Data Preview:\n    State_x   County  Year  ...  State_y  Region  Zone\n0   Alabama  Baldwin  2017  ...  Alabama       4     3\n5   Alabama     Clay  2017  ...  Alabama       4     1\n10  Alabama  Colbert  2017  ...  Alabama       4     1\n15  Alabama   DeKalb  2017  ...      NaN     NaN   NaN\n16  Alabama   Elmore  2017  ...  Alabama       4     2\n[5 rows x 30 columns]\n",
  "history_begin_time" : 1741882824505,
  "history_end_time" : 1741882825279,
  "history_notes" : null,
  "history_process" : "7my0ib",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "v6bsa3wdcem",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1741882825260,
  "history_notes" : null,
  "history_process" : "kth1gk",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jlo790lwokq",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1741882825276,
  "history_notes" : null,
  "history_process" : "hkyofs",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "kllk8drzzrh",
  "history_input" : "import os\nimport urllib.request\nimport urllib.parse\n\ndef download_file(url, save_location, file_name=\"incd.csv\"):\n    \"\"\"\n    Downloads a file from a given URL and saves it to the specified location.\n    \n    Args:\n        url (str): The URL of the file to download.\n        save_location (str): The directory where the file should be saved.\n        file_name (str): The name to save the file as (default is 'incd.csv').\n    \n    Returns:\n        str: Full path of the saved file if successful, None if failed.\n    \"\"\"\n    try:\n        print(\"Starting file download...\")\n        \n        # Ensure the save directory exists\n        os.makedirs(save_location, exist_ok=True)\n\n        # Open the URL and read the content\n        with urllib.request.urlopen(url) as response:\n            if response.status != 200:\n                print(f\"Failed to download file. HTTP Status: {response.status}\")\n                return None\n            file_content = response.read()\n        \n        # Define the full save path\n        save_path = os.path.join(save_location, file_name)\n\n        # Write the file\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n        return save_path  # Return the saved file path\n\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n        return None\n",
  "history_output" : "",
  "history_begin_time" : 1741882802894,
  "history_end_time" : 1741882804024,
  "history_notes" : null,
  "history_process" : "dv6ldm",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "8xv2q14s579",
  "history_input" : "import os\nimport pandas as pd\nimport zipfile\nfrom download_data_utils import download_file  \n\n# Define the base directory to save files\nTARGET_DIRECTORY = \"/media/volume1\"\n\n# Ensure the directory exists\nos.makedirs(TARGET_DIRECTORY, exist_ok=True)\n\n# Dictionary of URLs for all years\nAQI_DATA_URLS = {\n    2017: \"https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2017.zip\",\n    2018: \"https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2018.zip\",\n    2019: \"https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2019.zip\",\n    2020: \"https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2020.zip\",\n    2021: \"https://aqs.epa.gov/aqsweb/airdata/annual_aqi_by_county_2021.zip\",\n}\n\ndef retrieve_aqi(year):\n    \"\"\"\n    Downloads the AQI ZIP file for a given year, extracts the CSV, and saves it to /media/volume1.\n    \"\"\"\n    zip_file_path = os.path.join(TARGET_DIRECTORY, f\"annual_aqi_by_county_{year}.zip\")\n    extracted_csv_path = os.path.join(TARGET_DIRECTORY, f\"annual_aqi_by_county_{year}.csv\")\n\n    # Get the download URL\n    original_url = AQI_DATA_URLS.get(year)\n    if not original_url:\n        print(f\"No URL found for year {year}. Skipping...\")\n        return\n\n    # Download the ZIP file\n    saved_file = download_file(original_url, TARGET_DIRECTORY, f\"annual_aqi_by_county_{year}.zip\")\n\n    if saved_file and os.path.exists(saved_file):\n        try:\n            # Extract the ZIP file\n            with zipfile.ZipFile(saved_file, \"r\") as zip_ref:\n                zip_ref.extractall(TARGET_DIRECTORY)\n                extracted_files = zip_ref.namelist()  # Get extracted file names\n\n            # Identify the correct CSV file\n            csv_file_name = next((f for f in extracted_files if f.endswith(\".csv\")), None)\n            if csv_file_name:\n                extracted_csv_path = os.path.join(TARGET_DIRECTORY, csv_file_name)\n\n                # Load CSV into pandas\n                df = pd.read_csv(extracted_csv_path)\n\n                # Print a few rows to verify\n                print(f\"AQI Data for {year} extracted and saved as: {extracted_csv_path}\")\n                print(\"\\nAQI Data Preview:\")\n                print(df.head())  # Prints the first 5 rows\n            else:\n                print(f\"No CSV file found in the ZIP archive for {year}.\")\n\n        except Exception as e:\n            print(f\"Error extracting or loading AQI Data for {year}: {e}\")\n    else:\n        print(f\"AQI data download failed for {year}. Please check the URL and permissions.\")\n\ndef retrieve_all_years():\n    \"\"\"\n    Downloads and processes AQI data for all specified years.\n    \"\"\"\n    for year in AQI_DATA_URLS.keys():\n        retrieve_aqi(year)\n\nif __name__ == \"__main__\":\n    retrieve_all_years()\n",
  "history_output" : "Starting file download...\nFile downloaded successfully and saved as: /media/volume1/annual_aqi_by_county_2017.zip\nAQI Data for 2017 extracted and saved as: /media/volume1/annual_aqi_by_county_2017.csv\nAQI Data Preview:\n     State   County  Year  ...  Days Ozone  Days PM2.5  Days PM10\n0  Alabama  Baldwin  2017  ...         188          82          0\n1  Alabama     Clay  2017  ...           0         118          0\n2  Alabama  Colbert  2017  ...         202          81          0\n3  Alabama   DeKalb  2017  ...         290          69          0\n4  Alabama   Elmore  2017  ...         226           0          0\n[5 rows x 18 columns]\nStarting file download...\nFile downloaded successfully and saved as: /media/volume1/annual_aqi_by_county_2018.zip\nAQI Data for 2018 extracted and saved as: /media/volume1/annual_aqi_by_county_2018.csv\nAQI Data Preview:\n     State   County  Year  ...  Days Ozone  Days PM2.5  Days PM10\n0  Alabama  Baldwin  2018  ...         194          76          0\n1  Alabama     Clay  2018  ...           0         110          0\n2  Alabama  Colbert  2018  ...         184          93          0\n3  Alabama   DeKalb  2018  ...         286          64          0\n4  Alabama   Elmore  2018  ...         222           0          0\n[5 rows x 18 columns]\nStarting file download...\nFile downloaded successfully and saved as: /media/volume1/annual_aqi_by_county_2019.zip\nAQI Data for 2019 extracted and saved as: /media/volume1/annual_aqi_by_county_2019.csv\nAQI Data Preview:\n     State   County  Year  ...  Days Ozone  Days PM2.5  Days PM10\n0  Alabama  Baldwin  2019  ...         198          73          0\n1  Alabama     Clay  2019  ...           0         107          0\n2  Alabama  Colbert  2019  ...         219          44          0\n3  Alabama   DeKalb  2019  ...         306          55          0\n4  Alabama   Elmore  2019  ...         228           0          0\n[5 rows x 18 columns]\nStarting file download...\nFile downloaded successfully and saved as: /media/volume1/annual_aqi_by_county_2020.zip\nAQI Data for 2020 extracted and saved as: /media/volume1/annual_aqi_by_county_2020.csv\nAQI Data Preview:\n     State   County  Year  ...  Days Ozone  Days PM2.5  Days PM10\n0  Alabama  Baldwin  2020  ...         178          91          0\n1  Alabama     Clay  2020  ...           0         105          0\n2  Alabama   DeKalb  2020  ...         309          55          0\n3  Alabama   Elmore  2020  ...         197           0          0\n4  Alabama   Etowah  2020  ...         191          85          0\n[5 rows x 18 columns]\nStarting file download...\nFile downloaded successfully and saved as: /media/volume1/annual_aqi_by_county_2021.zip\nAQI Data for 2021 extracted and saved as: /media/volume1/annual_aqi_by_county_2021.csv\nAQI Data Preview:\n     State   County  Year  ...  Days Ozone  Days PM2.5  Days PM10\n0  Alabama  Baldwin  2021  ...         199          81          0\n1  Alabama     Clay  2021  ...           0         110          0\n2  Alabama   DeKalb  2021  ...         298          63          0\n3  Alabama   Elmore  2021  ...         241           0          0\n4  Alabama   Etowah  2021  ...         201          85          0\n[5 rows x 18 columns]\n",
  "history_begin_time" : 1741882819563,
  "history_end_time" : 1741882820780,
  "history_notes" : null,
  "history_process" : "pbr25n",
  "host_id" : "100001",
  "indicator" : "Done"
}]